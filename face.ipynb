{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for two images\n",
    "\n",
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "import numpy as np\n",
    "from keras_vggface import VGGFace\n",
    "from keras.preprocessing import image\n",
    "from keras_vggface import utils\n",
    "import keras\n",
    "import unittest\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def cos_similar(a, b):\n",
    "    A = np.array(a)\n",
    "    B = np.array(b)\n",
    "    \n",
    "    num = np.sum(A*B)\n",
    "    denom = np.linalg.norm(A) * np.linalg.norm(B)  \n",
    "    \n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    \n",
    "    return sim\n",
    "\n",
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "\n",
    "result = {}\n",
    "total = 0\n",
    "threshold = 0.86\n",
    "error = 0\n",
    "error_names = []\n",
    "test_num = 90\n",
    "\n",
    "batch_size = 50\n",
    "batches = 10\n",
    "\n",
    "img1_path = \"/home/thetensortec/TestDataSet_chip_mtcnn/TestDataSet_5/TestDataSet_5_scale_0.3/michael/149.jpg_chip.png\"\n",
    "img2_path = \"/home/thetensortec/TestDataSet_chip_mtcnn/TestDataSet_2/TestDataSet_2_scale_0.3/michael/150.jpg_chip.png\"\n",
    "\n",
    "img1 = image.load_img(img1_path, target_size=(224, 224))\n",
    "x1 = image.img_to_array(img1)\n",
    "x1 = np.expand_dims(x1, axis=0)\n",
    "x1 = utils.preprocess_input(x1, version=2)\n",
    "\n",
    "img2 = image.load_img(img2_path, target_size=(224, 224))\n",
    "x2 = image.img_to_array(img2)\n",
    "x2 = np.expand_dims(x2, axis=0)\n",
    "x2 = utils.preprocess_input(x2, version=2)\n",
    "\n",
    "preds_1 = model.predict(x1)\n",
    "preds_2 = model.predict(x2)\n",
    "\n",
    "\n",
    "preds_2 = np.array(preds_2)\n",
    "preds_2 = preds_2.reshape(2048)\n",
    "preds_1 = np.array(preds_1)\n",
    "preds_1 = preds_1.reshape(2048)\n",
    "\n",
    "similar = 1/(1+np.linalg.norm(preds_1-preds_2))\n",
    "cos = cos_similar(preds_1, preds_2)\n",
    "\n",
    "print (\"Similar is {}, between {} and {}\".format(cos, img1_path, img2_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for same person\n",
    "\n",
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "import numpy as np\n",
    "from keras_vggface import VGGFace\n",
    "from keras.preprocessing import image\n",
    "from keras_vggface import utils\n",
    "import keras\n",
    "import unittest\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def cos_similar(a, b):\n",
    "    A = np.array(a)\n",
    "    B = np.array(b)\n",
    "    \n",
    "    num = np.sum(A*B)\n",
    "    denom = np.linalg.norm(A) * np.linalg.norm(B)  \n",
    "    \n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    \n",
    "    return sim\n",
    "\n",
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_name = \"SamePerson_\" +  name + \"_margin_\" + str(threshold) + \"_scale_\" + scale + \".log\"\n",
    "\n",
    "test_data_path = \"/home/thetensortec/TestDataSet_chip_mtcnn/TestDataSet_Total/TestDataSet_Total_scale_0.3\"\n",
    "\n",
    "data_dirs = os.listdir(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = {}\n",
    "total = 0\n",
    "error = 0\n",
    "error_names = []\n",
    "test_num = 90\n",
    "error_similar_same = []\n",
    "error_similar_diff = []\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "name = 'zhaohuiwu'\n",
    "scale = '0.3'\n",
    "threshold = 0.8\n",
    "#thresholds = [0.78, 0.79, 0.80, 0.81, 0.82]\n",
    "\n",
    "log_file_name = \"SamePerson_\" +  name + \"_margin_\" + str(threshold) + \"_scale_\" + scale + \".log\"\n",
    "test_data_path = \"/home/thetensortec/TestDataSet_chip_mtcnn/SamePerson/\" + name + \"/\" + scale + \"/\"\n",
    "\n",
    "data_dirs = os.listdir(test_data_path)\n",
    "#data_dirs = ['TestDataSet_1', 'TestDataSet_2']\n",
    "## get all vectors\n",
    "data_vectors = {}\n",
    "dir_files = {}\n",
    "\n",
    "def cos_similar(a, b):\n",
    "    A = np.array(a)\n",
    "    B = np.array(b)\n",
    "    \n",
    "    num = np.sum(A*B)\n",
    "    denom = np.linalg.norm(A) * np.linalg.norm(B)  \n",
    "    \n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    \n",
    "    return sim\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    files = os.listdir(test_data_path + data_dir)\n",
    "    np.random.shuffle(files)\n",
    "    #files = files[:244]\n",
    "    dir_files[data_dir] = files\n",
    "    ## read images\n",
    "    #print (\"Loading...\")\n",
    "    imgs = []\n",
    "    for file in files:\n",
    "        file_path = test_data_path + data_dir + '/' + file\n",
    "        img = image.load_img(file_path, target_size=(224, 224))\n",
    "        img = image.img_to_array(img)\n",
    "        imgs.append(img)\n",
    "\n",
    "    ## predict\n",
    "    #print (\"Predicting...\")\n",
    "    vectors = []\n",
    "    for batch_start in np.arange(0, len(files), batch_size):\n",
    "        batch_end = batch_start + batch_size\n",
    "\n",
    "        if batch_end > len(files):\n",
    "            batch_end = len(files)\n",
    "        #print (\"Start {}, End {}\".format(batch_start, batch_end))\n",
    "        imgs_prepro = utils.preprocess_input(imgs[batch_start:batch_end], version=2)\n",
    "        imgs_predicts = model.predict(imgs_prepro)\n",
    "\n",
    "        vectors.extend(imgs_predicts)\n",
    "\n",
    "    data_vectors[data_dir] = vectors\n",
    "\n",
    "## compare all dirs\n",
    "#print (\"Comparing...\")\n",
    "all_similars = {}\n",
    "error_count = {}\n",
    "dirs_error_count = {}\n",
    "with open(log_file_name, \"w\") as fo:\n",
    "    for index1 in range(len(data_dirs)):\n",
    "        for index2 in range(index1 + 1, len(data_dirs)):\n",
    "            ## compare two dirs\n",
    "            similars = []\n",
    "\n",
    "            sub_total = 0\n",
    "            sub_error = 0\n",
    "\n",
    "            name1 = data_dirs[index1]\n",
    "            name2 = data_dirs[index2]\n",
    "\n",
    "            vectors1 = data_vectors[name1]\n",
    "            vectors2 = data_vectors[name2]\n",
    "\n",
    "            for vec1_index, vec1 in enumerate(vectors1):\n",
    "                for vec2_index, vec2 in enumerate(vectors2):\n",
    "                    cos = cos_similar(vec1, vec2)\n",
    "                    similars.append(cos)\n",
    "                    \n",
    "                    sub_total += 1\n",
    "\n",
    "                    if cos < threshold:\n",
    "                        sub_error += 1\n",
    "                        \n",
    "                        ## error count\n",
    "                        if name1 not in error_count.keys():\n",
    "                            error_count[name1] = {}\n",
    "                        if dir_files[name1][vec1_index] not in error_count[name1].keys():\n",
    "                            error_count[name1][dir_files[name1][vec1_index]] = 0\n",
    "                        if name1 + '/' + dir_files[name1][vec1_index] not in dirs_error_count.keys():\n",
    "                            dirs_error_count[name1 + '/' + dir_files[name1][vec1_index]] = 0\n",
    "                        \n",
    "                        if name2 not in error_count.keys():\n",
    "                            error_count[name2] = {}\n",
    "                        if dir_files[name2][vec2_index] not in error_count[name2].keys():\n",
    "                            error_count[name2][dir_files[name2][vec2_index]] = 0\n",
    "                        if name2 + '/' + dir_files[name2][vec2_index] not in dirs_error_count.keys():\n",
    "                            dirs_error_count[name2 + '/' + dir_files[name2][vec2_index]] = 0                            \n",
    "                            \n",
    "                        error_count[name1][dir_files[name1][vec1_index]] += 1\n",
    "                        error_count[name2][dir_files[name2][vec2_index]] += 1\n",
    "                        \n",
    "                        dirs_error_count[name1 + '/' + dir_files[name1][vec1_index]] += 1\n",
    "                        dirs_error_count[name2 + '/' + dir_files[name2][vec2_index]] += 1\n",
    "                        \n",
    "                        log = \"similar {}, {} --> {}\".format(cos, name1 + '/' + dir_files[name1][vec1_index], name2 + '/' + dir_files[name2][vec2_index])\n",
    "                        fo.write(log+'\\n')\n",
    "                        \n",
    "            #log = \"{} and {} Total compare {}, error {}, accuracy {}%\".format(name1, name2, sub_total, sub_error, round(((sub_total-sub_error)/sub_total)*100, 2))\n",
    "            #print (log)\n",
    "            #fo.write(log+'\\n')\n",
    "            all_similars[name1+'_'+name2] = similars\n",
    "\n",
    "            total += sub_total\n",
    "            error += sub_error\n",
    "\n",
    "    log = \"{} scale {} threshold {} Total num:{}, Errors:{}, Accuracy:{}%\".format(name, scale, threshold, total, error, round(((total-error)/total)*100, 2))\n",
    "    fo.write(log)\n",
    "    print (log)\n",
    "    print ('\\n')\n",
    "    #    for log in error_names:\n",
    "    #        fo.write(log+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 50\n",
    "\n",
    "print (\"LOG FILE: {}\".format(log_file_name))\n",
    "\n",
    "for key_dir in error_count.keys():\n",
    "    dis = sorted(error_count[key_dir].items(),key = lambda x:x[1],reverse = True)\n",
    "    print (\"{} top {}\".format(key_dir, top))\n",
    "    print (dis[:top])\n",
    "\n",
    "\n",
    "dis = sorted(dirs_error_count.items(),key = lambda x:x[1],reverse = True)\n",
    "print (\"All top {}\".format(top))\n",
    "for item in dis[:top]:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "data = np.random.randn(10000)\n",
    "\n",
    "#title = \"euclidean\"\n",
    "title = \"cos_similar\"\n",
    "name2 = \"qiuyifan\"\n",
    "name1 = \"fuxiaotian\"\n",
    "\n",
    "#print (\"Total num:{}, Errors:{}, Accuracy:{}%\".format(total, error, round(((total-error)/total)*100, 2)))\n",
    "#print (error_names[:10])\n",
    "\n",
    "tmp = np.array(list(all_similars.keys()))\n",
    "\n",
    "plt.hist(all_similars[tmp[0]], bins=100, normed=0, facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "#plt.hist(cos_similars, bins=100, normed=0, facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "# 显示横轴标签\n",
    "plt.xlabel(\"range\")\n",
    "# 显示纵轴标签\n",
    "plt.ylabel(\"rate\")\n",
    "# 显示图标题\n",
    "plt.title(\"{} {} (scale {}, threshold {}) : {}\".format(name, title, scale, threshold, tmp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for many persones\n",
    "\n",
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "import numpy as np\n",
    "from keras_vggface import VGGFace\n",
    "from keras.preprocessing import image\n",
    "from keras_vggface import utils\n",
    "import keras\n",
    "import unittest\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def cos_similar(a, b):\n",
    "    A = np.array(a)\n",
    "    B = np.array(b)\n",
    "    \n",
    "    num = np.sum(A*B, axis=1)\n",
    "    denom = np.linalg.norm(A, axis=1) * np.linalg.norm(B, axis=1)  \n",
    "    \n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    \n",
    "    return sim\n",
    "\n",
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "result = {}\n",
    "total = 0\n",
    "error = 0\n",
    "error_names = []\n",
    "test_num = 90\n",
    "error_similar_same = []\n",
    "error_similar_diff = []\n",
    "\n",
    "batch_size = 50\n",
    "batches = 9\n",
    "\n",
    "dataset = 'Wild'\n",
    "threshold = 0.81\n",
    "scale = '0.2'\n",
    "\n",
    "\n",
    "log_file_name = \"TestDataSet_\" +  dataset + \"_margin_\" + str(threshold) + \"_scale_\" + scale + \".log\"\n",
    "test_data_path = \"/home/thetensortec/TestDataSet_chip_mtcnn/TestDataSet_\" +dataset+ \"/TestDataSet_\" +dataset+ \"_scale_\" + scale + \"/\"\n",
    "\n",
    "names = os.listdir(test_data_path)\n",
    "\n",
    "for index1 in range(len(names)):\n",
    "    sub_result = {}\n",
    "    result[names[index1]] = sub_result\n",
    "    \n",
    "    for index2 in range(index1 + 1):\n",
    "        path1 = test_data_path + names[index1] + \"/\"\n",
    "        path2 = test_data_path + names[index2] + \"/\"\n",
    "\n",
    "        files1 = os.listdir(path1)\n",
    "        files2 = os.listdir(path2)\n",
    "        \n",
    "        #print (path1)\n",
    "        #print (\"files1 num {}, files2 {}\".format(len(files1), len(files2)))\n",
    "        random.shuffle(files1)\n",
    "        random.shuffle(files2)\n",
    "\n",
    "        similars = []\n",
    "        cos_similars = []\n",
    "        peple_error_names = []\n",
    "        peple_preds1 = []\n",
    "        peple_preds2 = []\n",
    "        result[names[index1]][names[index2]] = {}\n",
    "\n",
    "        for batch in range(batches):\n",
    "            time_start = time.time()\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            # read batch files\n",
    "            imgs_1 = np.zeros((batch_size, 224, 224, 3))\n",
    "            imgs_2 = np.zeros((batch_size, 224, 224, 3))\n",
    "            \n",
    "            for index in range(start, end):\n",
    "                img1 = image.load_img(path1+files1[index], target_size=(224, 224))\n",
    "                x1 = image.img_to_array(img1)\n",
    "                imgs_1[index - batch*batch_size] = x1\n",
    "                \n",
    "                img2 = image.load_img(path2+files2[index], target_size=(224, 224))\n",
    "                x2 = image.img_to_array(img2)\n",
    "                imgs_2[index - batch*batch_size] = x2\n",
    "            \n",
    "            # preprocessing and predict\n",
    "            imgs_1 = utils.preprocess_input(imgs_1, version=2)\n",
    "            preds_1 = model.predict(imgs_1)\n",
    "            \n",
    "            imgs_2 = utils.preprocess_input(imgs_2, version=2)\n",
    "            preds_2 = model.predict(imgs_2)\n",
    "            \n",
    "            time_end = time.time()\n",
    "            \n",
    "            # append the similarity\n",
    "            peple_preds1.extend(preds_1)\n",
    "            peple_preds2.extend(preds_2)\n",
    "            \n",
    "        # calculate distance\n",
    "        assert(len(peple_preds1) == batch_size*batches)\n",
    "        \n",
    "        peple_preds_len = len(peple_preds1)\n",
    "        shift_preds1 = np.array(peple_preds1)\n",
    "        \n",
    "        for shift_b in range(peple_preds_len):\n",
    "            if shift_b == 0:\n",
    "                shift_preds2 = np.array(peple_preds2)\n",
    "            else:\n",
    "                shift_preds2 = np.append(peple_preds2[peple_preds_len-shift_b:][0:shift_b], peple_preds2[:peple_preds_len-shift_b], axis=0)\n",
    "\n",
    "            similar = 1/(1+np.linalg.norm(shift_preds1-shift_preds2, axis=1))\n",
    "            cos = cos_similar(shift_preds1, shift_preds2)\n",
    "\n",
    "            similars.extend(similar)\n",
    "            cos_similars.extend(cos)\n",
    "\n",
    "            total += peple_preds_len\n",
    "\n",
    "            index_array_a = np.array(np.arange(0, peple_preds_len))\n",
    "            index_array_b = np.array(np.arange(0, peple_preds_len))\n",
    "            index_array_b = np.append(index_array_b[len(index_array_b)-shift_b:][0:shift_b], index_array_b[:len(index_array_b)-shift_b])\n",
    "\n",
    "            for check_index in range(len(cos)):\n",
    "                if cos[check_index] >= threshold: # same person\n",
    "                    if index1 != index2:\n",
    "                        error += 1\n",
    "                        error_message = \"{} -->  {}, similar:{}\".format(names[index1] + \"/\" +files1[index_array_a[check_index]], names[index2] + \"/\" +files2[index_array_b[check_index]], cos[check_index])\n",
    "                        error_names.append(error_message)\n",
    "                        peple_error_names.append(error_message)\n",
    "                        error_similar_diff.append(cos[check_index])\n",
    "                else:\n",
    "                    if index1 == index2:\n",
    "                        error += 1\n",
    "                        error_message = \"{} -->  {}, similar:{}\".format(names[index1] + \"/\"+files1[index_array_a[check_index]], names[index2] + \"/\"+files2[index_array_b[check_index]], cos[check_index])\n",
    "                        error_names.append(error_message)\n",
    "                        peple_error_names.append(error_message)\n",
    "                        error_similar_same.append(cos[check_index])\n",
    "            #print (\"Batch size:{}  time cost:{} ms\".format(batch_size, (time_end-time_start)*1000))\n",
    "            \n",
    "        similars = np.array(similars)\n",
    "        cos_similars = np.array(cos_similars)\n",
    "        peple_error_names = np.array(peple_error_names)\n",
    "        \n",
    "        print (\"{} and {} has compared {} times\".format(names[index1], names[index2], len(similars)))\n",
    "        #print (\"min:{} max:{} mean:{}\".format(similars.min(), similars.max(), similars.mean()))\n",
    "        #print (\"min:{} max:{} mean:{}\".format(cos_similars.min(), cos_similars.max(), cos_similars.mean()))\n",
    "        \n",
    "        result[names[index1]][names[index2]][\"euclidean\"] = similars\n",
    "        result[names[index1]][names[index2]][\"cos_similar\"] = cos_similars\n",
    "        result[names[index1]][names[index2]][\"error_names\"] = peple_error_names\n",
    "        \n",
    "        #print (\"{} --> {}\".format(names[index1], names[index2]))\n",
    "\n",
    "with open(log_file_name, \"w\") as fo:\n",
    "    for log in error_names:\n",
    "        fo.write(log+'\\n')\n",
    "    fo.write(\"Total num:{}, Errors:{}, Accuracy:{}%\".format(total, error, round(((total-error)/total)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overlaping Count\n",
    "\n",
    "title = \"cos_similar\"\n",
    "name1 = ''\n",
    "name2 = ''\n",
    "\n",
    "for name1 in result.keys():\n",
    "    for name2 in result[name1].keys():\n",
    "\n",
    "        positive = np.array(result[name1][name1][title])\n",
    "        negative = np.array(result[name1][name2][title])\n",
    "\n",
    "        min_val = min(positive)\n",
    "        max_val = max(negative)\n",
    "\n",
    "        print (\"min:{} max:{}\".format(min_val, max_val))\n",
    "        #print (((positive>min_val) & (positive < max_val)).sum())\n",
    "\n",
    "        positive_overlap_count = ((positive >= min_val) & (positive <= max_val)).sum()\n",
    "        negative_overlap_count = ((negative >= min_val) & (negative <= max_val)).sum()\n",
    "\n",
    "        print (\"Between {} and {}, Total count {}, Overlap count {}\".format(name1, name4, len(positive)+len(negative), positive_overlap_count+negative_overlap_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "data = np.random.randn(10000)\n",
    "\n",
    "#title = \"euclidean\"\n",
    "title = \"cos_similar\"\n",
    "name2 = \"qiuyifan\"\n",
    "name1 = \"fuxiaotian\"\n",
    "\n",
    "print (\"Total num:{}, Errors:{}, Accuracy:{}%\".format(total, error, round(((total-error)/total)*100, 2)))\n",
    "\n",
    "#print (error_names[:10])\n",
    "\n",
    "plt.hist(result[name1][name2][title], bins=100, normed=0, facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "#plt.hist(cos_similars, bins=100, normed=0, facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "# 显示横轴标签\n",
    "plt.xlabel(\"range\")\n",
    "# 显示纵轴标签\n",
    "plt.ylabel(\"rate\")\n",
    "# 显示图标题\n",
    "plt.title(\"{} : {} and {}\".format(title, name1, name2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "data = np.random.randn(10000)\n",
    "\n",
    "#title = \"euclidean\"\n",
    "title = \"cos_similar\"\n",
    "name1 = \"wangmengmeng\"\n",
    "name2 = \"wangmengmeng\"\n",
    "name3 = \"wangmengmeng\"\n",
    "name4 = \"fuxiaotian\"\n",
    "\n",
    "plt.hist(result[name1][name2][title], bins=25, normed=0, facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.hist(result[name3][name4][title], bins=25, normed=0, facecolor=\"red\", edgecolor=\"red\", alpha=0.7)\n",
    "#plt.hist(cos_similars, bins=100, normed=0, facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "# 显示横轴标签\n",
    "plt.xlabel(\"Similarity\")\n",
    "# 显示纵轴标签\n",
    "plt.ylabel(\"Numbers\")\n",
    "# 显示图标题\n",
    "plt.title(\"Red:{} vs {}        Blue:{} vs {}\".format(name3, name4, name1, name2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface import utils\n",
    "\n",
    "# tensorflow\n",
    "model = VGGFace(model='resnet50', input_shape=(224, 224, 3))\n",
    "\n",
    "# Change the image path with yours.\n",
    "img = image.load_img('image/ajb.jpg', target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = utils.preprocess_input(x, version=2) # or version=2\n",
    "preds = model.predict(x)\n",
    "print('Predicted:', utils.decode_predictions(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for many persones\n",
    "\n",
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "import numpy as np\n",
    "from keras_vggface import VGGFace\n",
    "from keras.preprocessing import image\n",
    "from keras_vggface import utils\n",
    "import keras\n",
    "import unittest\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def cos_similar(a, b):\n",
    "    A = np.array(a)\n",
    "    B = np.array(b)\n",
    "    \n",
    "    num = np.sum(A*B, axis=1)\n",
    "    denom = np.linalg.norm(A, axis=1) * np.linalg.norm(B, axis=1)  \n",
    "    \n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    \n",
    "    return sim\n",
    "\n",
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "FF_result = []\n",
    "\n",
    "for threshold in [0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85]:\n",
    "    start = time.time()\n",
    "\n",
    "    ## For ROC line\n",
    "    result = {}\n",
    "    total = 0\n",
    "    error = 0\n",
    "    error_names = []\n",
    "    test_num = 90\n",
    "    error_similar_same = []\n",
    "    error_similar_diff = []\n",
    "\n",
    "    ROC_count = {}\n",
    "    ROC_count['diff_error'] = 0\n",
    "    ROC_count['same_error'] = 0\n",
    "    ## total = total_same_count + total_diff_count\n",
    "    ROC_count['total'] = 0\n",
    "    ROC_count['total_same_count'] = 0\n",
    "    ROC_count['total_diff_count'] = 0\n",
    "\n",
    "    batch_size = 70\n",
    "    batches = 2\n",
    "\n",
    "    log_file_name = str(threshold) + \"_Roc.log\"\n",
    "    test_data_path = '/home/thetensortec/TestDataSet_chip_mtcnn/TestDataSet_Total/TestDataSet_Total_scale_0.3/'\n",
    "\n",
    "    names = os.listdir(test_data_path)\n",
    "\n",
    "    for index1 in range(len(names)):\n",
    "        sub_result = {}\n",
    "        result[names[index1]] = sub_result\n",
    "\n",
    "        for index2 in range(index1 + 1):\n",
    "            path1 = test_data_path + names[index1] + \"/\"\n",
    "            path2 = test_data_path + names[index2] + \"/\"\n",
    "\n",
    "            files1 = os.listdir(path1)\n",
    "            files2 = os.listdir(path2)\n",
    "\n",
    "            #print (path1)\n",
    "            #print (\"files1 num {}, files2 {}\".format(len(files1), len(files2)))\n",
    "            random.shuffle(files1)\n",
    "            random.shuffle(files2)\n",
    "\n",
    "            similars = []\n",
    "            cos_similars = []\n",
    "            peple_error_names = []\n",
    "            peple_preds1 = []\n",
    "            peple_preds2 = []\n",
    "            result[names[index1]][names[index2]] = {}\n",
    "\n",
    "            for batch in range(batches):\n",
    "                time_start = time.time()\n",
    "                start = batch * batch_size\n",
    "                end = (batch + 1) * batch_size\n",
    "                # read batch files\n",
    "                imgs_1 = np.zeros((batch_size, 224, 224, 3))\n",
    "                imgs_2 = np.zeros((batch_size, 224, 224, 3))\n",
    "\n",
    "                for index in range(start, end):\n",
    "                    img1 = image.load_img(path1+files1[index], target_size=(224, 224))\n",
    "                    x1 = image.img_to_array(img1)\n",
    "                    imgs_1[index - batch*batch_size] = x1\n",
    "\n",
    "                    img2 = image.load_img(path2+files2[index], target_size=(224, 224))\n",
    "                    x2 = image.img_to_array(img2)\n",
    "                    imgs_2[index - batch*batch_size] = x2\n",
    "\n",
    "                # preprocessing and predict\n",
    "                imgs_1 = utils.preprocess_input(imgs_1, version=2)\n",
    "                preds_1 = model.predict(imgs_1)\n",
    "\n",
    "                imgs_2 = utils.preprocess_input(imgs_2, version=2)\n",
    "                preds_2 = model.predict(imgs_2)\n",
    "\n",
    "                time_end = time.time()\n",
    "\n",
    "                # append the similarity\n",
    "                peple_preds1.extend(preds_1)\n",
    "                peple_preds2.extend(preds_2)\n",
    "\n",
    "            # calculate distance\n",
    "            assert(len(peple_preds1) == batch_size*batches)\n",
    "\n",
    "            peple_preds_len = len(peple_preds1)\n",
    "            shift_preds1 = np.array(peple_preds1)\n",
    "\n",
    "            for shift_b in range(peple_preds_len):\n",
    "                if shift_b == 0:\n",
    "                    shift_preds2 = np.array(peple_preds2)\n",
    "                else:\n",
    "                    shift_preds2 = np.append(peple_preds2[peple_preds_len-shift_b:][0:shift_b], peple_preds2[:peple_preds_len-shift_b], axis=0)\n",
    "\n",
    "                similar = 1/(1+np.linalg.norm(shift_preds1-shift_preds2, axis=1))\n",
    "                cos = cos_similar(shift_preds1, shift_preds2)\n",
    "\n",
    "                similars.extend(similar)\n",
    "                cos_similars.extend(cos)\n",
    "\n",
    "                total += peple_preds_len\n",
    "                ROC_count['total'] += peple_preds_len\n",
    "\n",
    "                if index1 == index2:\n",
    "                    ROC_count['total_same_count'] += peple_preds_len\n",
    "                else:\n",
    "                    ROC_count['total_diff_count'] += peple_preds_len\n",
    "\n",
    "                index_array_a = np.array(np.arange(0, peple_preds_len))\n",
    "                index_array_b = np.array(np.arange(0, peple_preds_len))\n",
    "                index_array_b = np.append(index_array_b[len(index_array_b)-shift_b:][0:shift_b], index_array_b[:len(index_array_b)-shift_b])\n",
    "\n",
    "                for check_index in range(len(cos)):\n",
    "                    if cos[check_index] >= threshold: # same person\n",
    "                        if index1 != index2:\n",
    "                            error += 1\n",
    "                            error_message = \"{} -->  {}, similar:{}\".format(names[index1] + \"/\" +files1[index_array_a[check_index]], names[index2] + \"/\" +files2[index_array_b[check_index]], cos[check_index])\n",
    "                            error_names.append(error_message)\n",
    "                            peple_error_names.append(error_message)\n",
    "                            error_similar_diff.append(cos[check_index])\n",
    "                            ROC_count['diff_error'] += 1\n",
    "                    else:\n",
    "                        if index1 == index2:\n",
    "                            error += 1\n",
    "                            error_message = \"{} -->  {}, similar:{}\".format(names[index1] + \"/\"+files1[index_array_a[check_index]], names[index2] + \"/\"+files2[index_array_b[check_index]], cos[check_index])\n",
    "                            error_names.append(error_message)\n",
    "                            peple_error_names.append(error_message)\n",
    "                            error_similar_same.append(cos[check_index])\n",
    "                            ROC_count['same_error'] += 1\n",
    "                #print (\"Batch size:{}  time cost:{} ms\".format(batch_size, (time_end-time_start)*1000))\n",
    "\n",
    "            similars = np.array(similars)\n",
    "            cos_similars = np.array(cos_similars)\n",
    "            peple_error_names = np.array(peple_error_names)\n",
    "\n",
    "            print (\"{} and {} has compared {} times\".format(names[index1], names[index2], len(similars)))\n",
    "            #print (\"min:{} max:{} mean:{}\".format(similars.min(), similars.max(), similars.mean()))\n",
    "            #print (\"min:{} max:{} mean:{}\".format(cos_similars.min(), cos_similars.max(), cos_similars.mean()))\n",
    "\n",
    "            result[names[index1]][names[index2]][\"euclidean\"] = similars\n",
    "            result[names[index1]][names[index2]][\"cos_similar\"] = cos_similars\n",
    "            result[names[index1]][names[index2]][\"error_names\"] = peple_error_names\n",
    "\n",
    "            #print (\"{} --> {}\".format(names[index1], names[index2]))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    assert(ROC_count['total'] == ROC_count['total_same_count'] + ROC_count['total_diff_count'])\n",
    "\n",
    "    FAR = ROC_count['diff_error'] / ROC_count['total_diff_count']\n",
    "    FRR = ROC_count['same_error'] / ROC_count['total_same_count']\n",
    "    \n",
    "    FF_result.append((threshold, FAR, FRR))\n",
    "    \n",
    "    with open(log_file_name, \"w\") as fo:\n",
    "        fo.write(\"{}s, FAR {}, FRR {}, Total {}, Different Count {}, Same Count {}, Different Error {}, Same Error {}\".format(end-start, FAR, FRR, ROC_count['total'], ROC_count['total_diff_count'], ROC_count['total_same_count'], ROC_count['diff_error'], ROC_count['same_error']))\n",
    "\n",
    "    print (\"{}s, FAR {}, FRR {},Total {}, Different Count {}, Same Count {}, Different Error {}, Same Error {}\".format(end-start, FAR, FRR, ROC_count['total'], ROC_count['total_diff_count'], ROC_count['total_same_count'], ROC_count['diff_error'], ROC_count['same_error']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "show = np.array([[1.42977648e-01, 1.36054422e-03],\n",
    "                 [9.70626822e-02, 3.04761905e-03],\n",
    "                 [6.11205053e-02, 5.74149660e-03],\n",
    "                 [3.45947522e-02, 9.93877551e-03],\n",
    "                 [1.72065112e-02, 1.60068027e-02],\n",
    "                 [7.53935860e-03, 2.47551020e-02],\n",
    "                 [3.05053450e-03, 3.45374150e-02],\n",
    "                 [1.33916424e-03, 4.52789116e-02],\n",
    "                 [5.68999028e-04, 5.69183673e-02],\n",
    "                 [1.89504373e-04, 7.03129252e-02],\n",
    "                 [4.32458698e-05, 8.75442177e-02]])\n",
    "\n",
    "ROC = np.array([[x[0], x[1]] for x in show])\n",
    "print (ROC)\n",
    "plt.plot(ROC[:, 1], ROC[:, 0], marker = \"o\" ,mec = \"k\" , mfc = \"b\", mew = 0.5)\n",
    "plt.plot(ROC[:, 1]+0.03, ROC[:, 0]+0.03, marker = \"o\" ,mec = \"k\" , mfc = \"b\", mew = 0.5)\n",
    "\n",
    "plt.ylabel('FAR')\n",
    "plt.xlabel('FRR')\n",
    "plt.title(\"ROC\")\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "show = np.array([[1.42977648e-01, 1.36054422e-03],\n",
    "                 [9.70626822e-02, 3.04761905e-03],\n",
    "                 [6.11205053e-02, 5.74149660e-03],\n",
    "                 [3.45947522e-02, 9.93877551e-03],\n",
    "                 [1.72065112e-02, 1.60068027e-02],\n",
    "                 [7.53935860e-03, 2.47551020e-02],\n",
    "                 [3.05053450e-03, 3.45374150e-02],\n",
    "                 [1.33916424e-03, 4.52789116e-02],\n",
    "                 [5.68999028e-04, 5.69183673e-02],\n",
    "                 [1.89504373e-04, 7.03129252e-02],\n",
    "                 [4.32458698e-05, 8.75442177e-02]])\n",
    "\n",
    "ROC = np.array([[x[0], x[1]] for x in show])\n",
    "print (ROC)\n",
    "plt.plot(ROC[:, 1], ROC[:, 0], marker = \"o\" ,mec = \"k\" , mfc = \"b\", mew = 0.5)\n",
    "plt.plot(ROC[:, 1]+0.03, ROC[:, 0]+0.03, marker = \"o\" ,mec = \"k\" , mfc = \"b\", mew = 0.5)\n",
    "\n",
    "plt.ylabel('FAR')\n",
    "plt.xlabel('FRR')\n",
    "plt.title(\"ROC\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "VGG_show = np.array([[1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [9.70626822e-02, 3.04761905e-03],\n",
    "                    [6.11205053e-02, 5.74149660e-03],\n",
    "                    [3.45947522e-02, 9.93877551e-03],\n",
    "                    [1.72065112e-02, 1.60068027e-02],\n",
    "                    [7.53935860e-03, 2.47551020e-02],\n",
    "                    [3.05053450e-03, 3.45374150e-02],\n",
    "                    [1.33916424e-03, 4.52789116e-02],\n",
    "                    [5.68999028e-04, 5.69183673e-02],\n",
    "                    [1.89504373e-04, 7.03129252e-02],\n",
    "                    [4.32458698e-05, 8.75442177e-02]])\n",
    "\n",
    "VGG_ROC = np.array([[x[0], x[1]] for x in VGG_show])\n",
    "VGG_FRR = VGG_ROC[:, 1]\n",
    "VGG_FAR = VGG_ROC[:, 0]\n",
    "\n",
    "thres = np.array(np.arange(0.60, 0.86, 0.01))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(thres, VGG_FRR,  label='VGGFACE_FRR', color = 'b')\n",
    "plt.plot(thres, VGG_FAR,  label='VGGFACE_FAR', color = 'b')\n",
    "\n",
    "MTCNN_show = np.array([[0.052, 0.007],\n",
    "[0.042, 0.009],\n",
    "[0.034, 0.011],\n",
    "[0.027, 0.014],\n",
    "[0.021, 0.016],\n",
    "[0.016, 0.019],\n",
    "[0.012, 0.022],\n",
    "[0.009, 0.025],\n",
    "[0.007, 0.029],\n",
    "[0.005, 0.033],\n",
    "[0.003, 0.039],\n",
    "[0.002, 0.044],\n",
    "[0.001, 0.051],\n",
    "[0.001, 0.057],\n",
    "[0.000, 0.065],\n",
    "[0.000, 0.072],\n",
    "[0.000, 0.081],\n",
    "[0.000, 0.091],\n",
    "[0.000, 0.103],\n",
    "[0.000, 0.115],\n",
    "[0.000, 0.129],\n",
    "[0.000, 0.145],\n",
    "[0.000, 0.163],\n",
    "[0.000, 0.184],\n",
    "[0.000, 0.207],\n",
    "[0.000, 0.232]])\n",
    "\n",
    "MTCNN_ROC = np.array([[x[0], x[1]] for x in MTCNN_show])\n",
    "MTCNN_FRR = MTCNN_ROC[:, 1]\n",
    "MTCNN_FAR = MTCNN_ROC[:, 0]\n",
    "\n",
    "plt.plot(thres, MTCNN_FRR,  label='MTCNN_FRR', color = 'r', linestyle = \"--\")\n",
    "plt.plot(thres, MTCNN_FAR,  label='MTCNN_FAR', color = 'r', linestyle = \"--\")\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.0,1),loc=1,borderaxespad=0.)\n",
    "\n",
    "\n",
    "plt.ylabel('EER/FAR/FRR')\n",
    "plt.xlabel('Threshold')\n",
    "plt.title(\"MTCNN and VGGFACE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for senet two images\n",
    "\n",
    "import caffe\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "#weights_file = 'caffe_model/senet50_ft_caffe/senet50_ft.caffemodel'\n",
    "#model_file = 'caffe_model/senet50_ft_caffe/senet50_ft_feature.prototxt'\n",
    "\n",
    "#weights_file = 'caffe_model/senet50_scratch_caffe/senet50_scratch.caffemodel'\n",
    "#model_file = 'caffe_model/senet50_scratch_caffe/senet50_scratch_feature.prototxt'\n",
    "\n",
    "#weights_file = 'caffe_model/resnet50_ft_caffe/resnet50_ft.caffemodel'\n",
    "#model_file = 'caffe_model/resnet50_ft_caffe/resnet50_ft_feature.prototxt'\n",
    "\n",
    "weights_file = 'caffe_model/resnet50_scratch_caffe/resnet50_scratch.caffemodel'\n",
    "model_file = 'caffe_model/resnet50_scratch_caffe/resnet50_scratch_feature.prototxt'\n",
    "\n",
    "\n",
    "caffe.set_mode_gpu()\n",
    "net = caffe.Net(model_file, weights_file, caffe.TEST)\n",
    "\n",
    "# create transformer for the input called 'data'\n",
    "mu = np.array([91.4953, 103.8827, 131.0912])\n",
    "\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR\n",
    "\n",
    "net.blobs['data'].reshape(batch_size,        # batch size\n",
    "                          3,         # 3-channel (BGR) images\n",
    "                          224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84799814 0.         0.9964056  ... 0.03545221 0.6139068  0.33769438]\n",
      "[0.02687134 0.         1.5585577  ... 0.01613929 0.8262487  0.36209238]\n",
      "cos Similar is 0.987234503031, ogi is 0.0692367337078\n"
     ]
    }
   ],
   "source": [
    "image1_path = \"image/8.jpg_chip.png\"\n",
    "image2_path = \"image/125.jpg_chip.png\"\n",
    "\n",
    "#image1_path = \"image/5.jpg\"\n",
    "#image2_path = \"image/124.jpg\"\n",
    "\n",
    "\n",
    "def cos_similar(a, b):\n",
    "    A = np.array(a)\n",
    "    B = np.array(b)\n",
    "    \n",
    "    num = np.sum(A*B)\n",
    "    denom = np.linalg.norm(A) * np.linalg.norm(B)  \n",
    "    \n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    \n",
    "    return sim\n",
    "\n",
    "## load and fill image\n",
    "image = caffe.io.load_image(image1_path)\n",
    "transformed_image = transformer.preprocess('data', image)\n",
    "net.blobs['data'].data[0] = transformed_image\n",
    "\n",
    "image = caffe.io.load_image(image2_path)\n",
    "transformed_image = transformer.preprocess('data', image)\n",
    "net.blobs['data'].data[1] = transformed_image\n",
    "\n",
    "## forward\n",
    "output = net.forward()\n",
    "output_features = output['flatten']  ## row features\n",
    "\n",
    "print (output_features[0])\n",
    "print (output_features[1])\n",
    "\n",
    "ogi = 1/(1+np.linalg.norm(output_features[0]-output_features[1]))\n",
    "\n",
    "print (\"cos Similar is {}, ogi is {}\".format(cos_similar(output_features[0], output_features[1]), ogi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for caffe final test\n",
    "\n",
    "import caffe\n",
    "import numpy as np\n",
    "\n",
    "def cos_similar(a, b):\n",
    "    A = np.array(a)\n",
    "    B = np.array(b)\n",
    "    \n",
    "    num = np.sum(A*B, axis=1)\n",
    "    denom = np.linalg.norm(A, axis=1) * np.linalg.norm(B, axis=1)  \n",
    "    \n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    \n",
    "    return sim\n",
    "\n",
    "batch_size = 35\n",
    "batches = 4\n",
    "\n",
    "alg = 'senet50_scratch'\n",
    "\n",
    "#weights_file = 'caffe_model/senet50_ft_caffe/senet50_ft.caffemodel'\n",
    "#model_file = 'caffe_model/senet50_ft_caffe/senet50_ft_feature.prototxt'\n",
    "\n",
    "weights_file = 'caffe_model/senet50_scratch_caffe/senet50_scratch.caffemodel'\n",
    "model_file = 'caffe_model/senet50_scratch_caffe/senet50_scratch_feature.prototxt'\n",
    "\n",
    "#weights_file = 'caffe_model/resnet50_ft_caffe/resnet50_ft.caffemodel'\n",
    "#model_file = 'caffe_model/resnet50_ft_caffe/resnet50_ft_feature.prototxt'\n",
    "\n",
    "#weights_file = 'caffe_model/resnet50_scratch_caffe/resnet50_scratch.caffemodel'\n",
    "#model_file = 'caffe_model/resnet50_scratch_caffe/resnet50_scratch_feature.prototxt'\n",
    "\n",
    "\n",
    "caffe.set_mode_gpu()\n",
    "net = caffe.Net(model_file, weights_file, caffe.TEST)\n",
    "\n",
    "# create transformer for the input called 'data'\n",
    "mu = np.array([91.4953, 103.8827, 131.0912])\n",
    "\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR\n",
    "\n",
    "net.blobs['data'].reshape(batch_size,        # batch size\n",
    "                          3,         # 3-channel (BGR) images\n",
    "                          224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to test wangmengmeng\n",
      "Start to test xiexiyu\n",
      "Start to test fuxiaotian\n",
      "Start to test maggie\n",
      "Start to test zengqi\n",
      "Start to test liuyang\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "FF_result = []\n",
    "\n",
    "for threshold in tqdm([0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85]):\n",
    "    start = time.time()\n",
    "\n",
    "    ## For ROC line\n",
    "    result = {}\n",
    "    total = 0\n",
    "    error = 0\n",
    "    error_names = []\n",
    "    test_num = 90\n",
    "    error_similar_same = []\n",
    "    error_similar_diff = []\n",
    "\n",
    "    ROC_count = {}\n",
    "    ROC_count['diff_error'] = 0\n",
    "    ROC_count['same_error'] = 0\n",
    "    ## total = total_same_count + total_diff_count\n",
    "    ROC_count['total'] = 0\n",
    "    ROC_count['total_same_count'] = 0\n",
    "    ROC_count['total_diff_count'] = 0\n",
    "\n",
    "    log_file_name = alg+'_'+str(threshold) + \"_Roc.log\"\n",
    "    test_data_path = '/home/ubuntu/workspace/tensortec/face/image/TestDataSet_Total/'\n",
    "\n",
    "    names = os.listdir(test_data_path)\n",
    "\n",
    "    for index1 in range(len(names)):\n",
    "        sub_result = {}\n",
    "        result[names[index1]] = sub_result\n",
    "        print (\"Start to test {}\".format(names[index1]))\n",
    "        for index2 in range(index1 + 1):\n",
    "            path1 = test_data_path + names[index1] + \"/\"\n",
    "            path2 = test_data_path + names[index2] + \"/\"\n",
    "\n",
    "            files1 = os.listdir(path1)\n",
    "            files2 = os.listdir(path2)\n",
    "\n",
    "            #print (path1)\n",
    "            #print (\"files1 num {}, files2 {}\".format(len(files1), len(files2)))\n",
    "            random.shuffle(files1)\n",
    "            random.shuffle(files2)\n",
    "\n",
    "            similars = []\n",
    "            cos_similars = []\n",
    "            peple_error_names = []\n",
    "            peple_preds1 = []\n",
    "            peple_preds2 = []\n",
    "            result[names[index1]][names[index2]] = {}\n",
    "\n",
    "            for batch in range(batches):\n",
    "                start = batch * batch_size\n",
    "                end = (batch + 1) * batch_size\n",
    "                # read batch files\n",
    "                imgs_1 = np.zeros((batch_size, 3, 224, 224))\n",
    "                imgs_2 = np.zeros((batch_size, 3, 224, 224))\n",
    "\n",
    "                for index in range(start, end):\n",
    "                    img1 = caffe.io.load_image(path1+files1[index])\n",
    "                    x1 = transformer.preprocess('data', img1)\n",
    "                    imgs_1[index - batch*batch_size] = x1\n",
    "                    \n",
    "                    img2 = caffe.io.load_image(path2+files2[index])\n",
    "                    x2 = transformer.preprocess('data', img2)\n",
    "                    imgs_2[index - batch*batch_size] = x2\n",
    "\n",
    "                # preprocessing and predict\n",
    "                net.blobs['data'].data[...] = imgs_1\n",
    "                output1 = net.forward()\n",
    "                preds_1 = output1['flatten'].copy()\n",
    "                \n",
    "                net.blobs['data'].data[...] = imgs_2\n",
    "                output2 = net.forward()\n",
    "                preds_2 = output2['flatten'].copy()\n",
    "                \n",
    "                # append the similarity\n",
    "                peple_preds1.extend(preds_1)\n",
    "                peple_preds2.extend(preds_2)\n",
    "\n",
    "            # calculate distance\n",
    "            assert(len(peple_preds1) == batch_size*batches)\n",
    "\n",
    "            peple_preds_len = len(peple_preds1)\n",
    "            shift_preds1 = np.array(peple_preds1)\n",
    "\n",
    "            for shift_b in range(peple_preds_len):\n",
    "                if shift_b == 0:\n",
    "                    shift_preds2 = np.array(peple_preds2)\n",
    "                else:\n",
    "                    shift_preds2 = np.append(peple_preds2[peple_preds_len-shift_b:][0:shift_b], peple_preds2[:peple_preds_len-shift_b], axis=0)\n",
    "\n",
    "                similar = 1/(1+np.linalg.norm(shift_preds1-shift_preds2, axis=1))\n",
    "                cos = cos_similar(shift_preds1, shift_preds2)\n",
    "\n",
    "                similars.extend(similar)\n",
    "                cos_similars.extend(cos)\n",
    "\n",
    "                total += peple_preds_len\n",
    "                ROC_count['total'] += peple_preds_len\n",
    "\n",
    "                if index1 == index2:\n",
    "                    ROC_count['total_same_count'] += peple_preds_len\n",
    "                else:\n",
    "                    ROC_count['total_diff_count'] += peple_preds_len\n",
    "\n",
    "                index_array_a = np.array(np.arange(0, peple_preds_len))\n",
    "                index_array_b = np.array(np.arange(0, peple_preds_len))\n",
    "                index_array_b = np.append(index_array_b[len(index_array_b)-shift_b:][0:shift_b], index_array_b[:len(index_array_b)-shift_b])\n",
    "\n",
    "                for check_index in range(len(cos)):\n",
    "                    if cos[check_index] >= threshold: # same person\n",
    "                        if index1 != index2:\n",
    "                            #error += 1\n",
    "                            #error_message = \"{} -->  {}, similar:{}\".format(names[index1] + \"/\" +files1[index_array_a[check_index]], names[index2] + \"/\" +files2[index_array_b[check_index]], cos[check_index])\n",
    "                            #error_names.append(error_message)\n",
    "                            #peple_error_names.append(error_message)\n",
    "                            #error_similar_diff.append(cos[check_index])\n",
    "                            ROC_count['diff_error'] += 1\n",
    "                    else:\n",
    "                        if index1 == index2:\n",
    "                            #error += 1\n",
    "                            #error_message = \"{} -->  {}, similar:{}\".format(names[index1] + \"/\"+files1[index_array_a[check_index]], names[index2] + \"/\"+files2[index_array_b[check_index]], cos[check_index])\n",
    "                            #error_names.append(error_message)\n",
    "                            #peple_error_names.append(error_message)\n",
    "                            #error_similar_same.append(cos[check_index])\n",
    "                            ROC_count['same_error'] += 1\n",
    "                #print (\"Batch size:{}  time cost:{} ms\".format(batch_size, (time_end-time_start)*1000))\n",
    "\n",
    "            #similars = np.array(similars)\n",
    "            #cos_similars = np.array(cos_similars)\n",
    "            #peple_error_names = np.array(peple_error_names)\n",
    "\n",
    "            #print (\"{} and {} has compared {} times\".format(names[index1], names[index2], len(similars)))\n",
    "            #print (\"min:{} max:{} mean:{}\".format(similars.min(), similars.max(), similars.mean()))\n",
    "            #print (\"min:{} max:{} mean:{}\".format(cos_similars.min(), cos_similars.max(), cos_similars.mean()))\n",
    "\n",
    "            #result[names[index1]][names[index2]][\"euclidean\"] = similars\n",
    "            #result[names[index1]][names[index2]][\"cos_similar\"] = cos_similars\n",
    "            #result[names[index1]][names[index2]][\"error_names\"] = peple_error_names\n",
    "\n",
    "            #print (\"{} --> {}\".format(names[index1], names[index2]))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    assert(ROC_count['total'] == ROC_count['total_same_count'] + ROC_count['total_diff_count'])\n",
    "\n",
    "    FAR = float(ROC_count['diff_error']) / float(ROC_count['total_diff_count'])\n",
    "    FRR = float(ROC_count['same_error']) / float(ROC_count['total_same_count'])\n",
    "    \n",
    "    FF_result.append((threshold, FAR, FRR))\n",
    "    \n",
    "    with open(log_file_name, \"w\") as fo:\n",
    "        fo.write(\"FAR {}, FRR {}, Total {}, Different Count {}, Same Count {}, Different Error {}, Same Error {}\".format(FAR, FRR, ROC_count['total'], ROC_count['total_diff_count'], ROC_count['total_same_count'], ROC_count['diff_error'], ROC_count['same_error']))\n",
    "\n",
    "    print (\"FAR {}, FRR {},Total {}, Different Count {}, Same Count {}, Different Error {}, Same Error {}\".format(FAR, FRR, ROC_count['total'], ROC_count['total_diff_count'], ROC_count['total_same_count'], ROC_count['diff_error'], ROC_count['same_error']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "### ================================ KERAS VGG FACE ================================ ##\n",
    "\n",
    "VGG_show = np.array([[1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [1.42977648e-01, 1.36054422e-03],\n",
    "                    [9.70626822e-02, 3.04761905e-03],\n",
    "                    [6.11205053e-02, 5.74149660e-03],\n",
    "                    [3.45947522e-02, 9.93877551e-03],\n",
    "                    [1.72065112e-02, 1.60068027e-02],\n",
    "                    [7.53935860e-03, 2.47551020e-02],\n",
    "                    [3.05053450e-03, 3.45374150e-02],\n",
    "                    [1.33916424e-03, 4.52789116e-02],\n",
    "                    [5.68999028e-04, 5.69183673e-02],\n",
    "                    [1.89504373e-04, 7.03129252e-02],\n",
    "                    [4.32458698e-05, 8.75442177e-02]])\n",
    "\n",
    "VGG_ROC = np.array([[x[0], x[1]] for x in VGG_show])\n",
    "VGG_FRR = VGG_ROC[:, 1]\n",
    "VGG_FAR = VGG_ROC[:, 0]\n",
    "\n",
    "thres = np.array(np.arange(0.60, 0.86, 0.01))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(thres, VGG_FRR,  label='VGGFACE_FRR', color = 'b')\n",
    "plt.plot(thres, VGG_FAR,  label='VGGFACE_FAR', color = 'b')\n",
    "\n",
    "### ======================MTCNN====================== ###\n",
    "\n",
    "MTCNN_show = np.array([[0.052, 0.007],\n",
    "[0.042, 0.009],\n",
    "[0.034, 0.011],\n",
    "[0.027, 0.014],\n",
    "[0.021, 0.016],\n",
    "[0.016, 0.019],\n",
    "[0.012, 0.022],\n",
    "[0.009, 0.025],\n",
    "[0.007, 0.029],\n",
    "[0.005, 0.033],\n",
    "[0.003, 0.039],\n",
    "[0.002, 0.044],\n",
    "[0.001, 0.051],\n",
    "[0.001, 0.057],\n",
    "[0.000, 0.065],\n",
    "[0.000, 0.072],\n",
    "[0.000, 0.081],\n",
    "[0.000, 0.091],\n",
    "[0.000, 0.103],\n",
    "[0.000, 0.115],\n",
    "[0.000, 0.129],\n",
    "[0.000, 0.145],\n",
    "[0.000, 0.163],\n",
    "[0.000, 0.184],\n",
    "[0.000, 0.207],\n",
    "[0.000, 0.232]])\n",
    "\n",
    "MTCNN_ROC = np.array([[x[0], x[1]] for x in MTCNN_show])\n",
    "MTCNN_FRR = MTCNN_ROC[:, 1]\n",
    "MTCNN_FAR = MTCNN_ROC[:, 0]\n",
    "\n",
    "plt.plot(thres, MTCNN_FRR,  label='resnet50_ft_FRR', color = 'g', linestyle = \"--\")\n",
    "plt.plot(thres, MTCNN_FAR,  label='resnet50_ft_FAR', color = 'g', linestyle = \"--\")\n",
    "\n",
    "### =========================== resnet50_ft =========================== ###\n",
    "resnet50_ft_show = np.array()\n",
    "\n",
    "resnet50_ft_ROC = np.array([[x[0], x[1]] for x in resnet50_ft_show])\n",
    "resnet50_ft_FRR = resnet50_ft_ROC[:, 1]\n",
    "resnet50_ft_FAR = resnet50_ft_ROC[:, 0]\n",
    "\n",
    "plt.plot(thres, resnet50_ft_FRR,  label='resnet50_ft_FRR', color = 'g', linestyle = \"--\")\n",
    "plt.plot(thres, resnet50_ft_FAR,  label='resnet50_ft_FAR', color = 'g', linestyle = \"--\")\n",
    "\n",
    "### =========================== SHOW ============================ ###\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.0,1),loc=1,borderaxespad=0.)\n",
    "\n",
    "\n",
    "plt.ylabel('EER/FAR/FRR')\n",
    "plt.xlabel('Threshold')\n",
    "plt.title(\"MTCNN and VGGFACEs\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
